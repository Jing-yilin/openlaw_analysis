import requests
import re
import json
import pandas as pd
import asyncio
import aiohttp
import os
from bs4 import BeautifulSoup
import sys
import pathlib
from tqdm import tqdm
import warnings
from tqdm.asyncio import tqdm_asyncio
from logging import Logger

warnings.filterwarnings("ignore")

sys.path.append(str(pathlib.Path(__file__).resolve().parent.parent))

from ..utils import create_dir
from ..chat_model import get_conversation_chain, LAW_RESULT_TEMPLATE


DOC_TYPE_MAP = {
    "": "",
    "é€šçŸ¥ä¹¦": "Notification",
    "åˆ¤å†³ä¹¦": "Verdict",
    "è°ƒè§£ä¹¦": "Mediation",
    "å†³å®šä¹¦": "Decision",
    "ä»¤": "Warrant",
    "è£å®šä¹¦": "Ruling",
    "å…¶ä»–": "Other",
}

PROCEDURE_TYPE_MAP = {
    "": "",
    "ä¸€å®¡": "First",
    "äºŒå®¡": "Second",
    "å†å®¡": "Retrial",
    "å¤æ ¸": "Review",
    "åˆ‘ç½šå˜æ›´": "PenaltyChange",
    "å…¶ä»–": "Other",
}

COURT_LEVEL_MAP = {
    "": "",
    "æœ€é«˜äººæ°‘æ³•é™¢": "0",
    "é«˜çº§äººæ°‘æ³•é™¢": "1",
    "ä¸­çº§äººæ°‘æ³•é™¢": "2",
    "åŸºå±‚äººæ°‘æ³•é™¢": "3",
}


JUDGE_RESULT_MAP = {
    "": "",
    "æœªçŸ¥": "Unknow",
    "å®Œå…¨æ”¯æŒ": "Victory",
    "éƒ¨åˆ†æ”¯æŒ": "Half",
    "ä¸æ”¯æŒ": "Half",
    "æ’¤é”€": "Half",
    "å…¶ä»–": "Other",
}


LITIGATION_TYPE_MAP = {
    "": "",
    "æ°‘äº‹": "Civil",
    "åˆ‘äº‹": "Criminal",
    "è¡Œæ”¿": "Administration",
    "èµ”å¿": "Compensation",
    "æ‰§è¡Œ": "Execution",
}

ZONE_MAP = {
    "": "",
    "åŒ—äº¬å¸‚": "åŒ—äº¬å¸‚",
    "å¤©æ´¥å¸‚": "å¤©æ´¥å¸‚",
    "æ²³åŒ—çœ": "æ²³åŒ—çœ",
    "å±±è¥¿çœ": "å±±è¥¿çœ",
    "å†…è’™å¤è‡ªæ²»åŒº": "å†…è’™å¤è‡ªæ²»åŒº",
    "è¾½å®çœ": "è¾½å®çœ",
    "å‰æ—çœ": "å‰æ—çœ",
    "é»‘é¾™æ±Ÿçœ": "é»‘é¾™æ±Ÿçœ",
    "ä¸Šæµ·å¸‚": "ä¸Šæµ·å¸‚",
    "æ±Ÿè‹çœ": "æ±Ÿè‹çœ",
    "æµ™æ±Ÿçœ": "æµ™æ±Ÿçœ",
    "å®‰å¾½çœ": "å®‰å¾½çœ",
    "ç¦å»ºçœ": "ç¦å»ºçœ",
    "æ±Ÿè¥¿çœ": "æ±Ÿè¥¿çœ",
    "å±±ä¸œçœ": "å±±ä¸œçœ",
    "æ²³å—çœ": "æ²³å—çœ",
    "æ¹–åŒ—çœ": "æ¹–åŒ—çœ",
    "æ¹–å—çœ": "æ¹–å—çœ",
    "å¹¿ä¸œçœ": "å¹¿ä¸œçœ",
    "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº": "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº",
    "æµ·å—çœ": "æµ·å—çœ",
    "é‡åº†å¸‚": "é‡åº†å¸‚",
    "å››å·çœ": "å››å·çœ",
    "è´µå·çœ": "è´µå·çœ",
    "äº‘å—çœ": "äº‘å—çœ",
    "è¥¿è—è‡ªæ²»åŒº": "è¥¿è—è‡ªæ²»åŒº",
    "é™•è¥¿çœ": "é™•è¥¿çœ",
    "ç”˜è‚ƒçœ": "ç”˜è‚ƒçœ",
    "é’æµ·çœ": "é’æµ·çœ",
    "å®å¤å›æ—è‡ªæ²»åŒº": "å®å¤å›æ—è‡ªæ²»åŒº",
    "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº": "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº",
    "å°æ¹¾çœ": "å°æ¹¾çœ",
    "é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº": "é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº",
    "æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº": "æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº",
}


class OpenLawSpider:
    def __init__(
        self,
        config: dict,
        links: dict = {},
        contents: list = [],
        session: aiohttp.ClientSession = None,
        concurrent: int = 50,
        ai_mode=True,
        logger: Logger = None,
    ):
        self.base_url = "http://openlaw.cn"

        self.config = config
        self.cookie = config["cookie"]
        self.user_agent = config["user_agent"]
        self.keyword = config["å…³é”®è¯"]
        self.litigation_type = LITIGATION_TYPE_MAP[config["æ¡ˆä»¶ç±»å‹"]]
        self.doc_type = DOC_TYPE_MAP[config["æ–‡ä¹¦ç±»å‹"]]
        self.zone = config["æ³•é™¢ï¼ˆåœ°åŒºï¼‰"]
        self.procedure_type = PROCEDURE_TYPE_MAP[config["å®¡åˆ¤ç¨‹åº"]]
        self.court_level = COURT_LEVEL_MAP[config["æ³•é™¢å±‚çº§"]]
        self.judge_result = JUDGE_RESULT_MAP[config["åˆ¤å†³ç»“æœ"]]
        self.judge_date_begin = config["åˆ¤å†³å¼€å§‹æ—¶é—´"]
        self.judge_date_end = config["åˆ¤å†³ç»“æŸæ—¶é—´"]
        self.num = config["æ•°é‡"]
        self.start_page = 1
        self.end_page = (self.num - 1) // 10 + 1

        self.headers = {
            "User-Agent": self.user_agent,
            "Cookie": self.cookie,
        }
        self.links = links
        self.contents = contents
        self.session = session
        self.concurrent = concurrent
        self.__base_dir = f'./data/{config["å…³é”®è¯"]}_{config["æ¡ˆä»¶ç±»å‹"]}_{config["æ–‡ä¹¦ç±»å‹"]}_{config["æ³•é™¢ï¼ˆåœ°åŒºï¼‰"]}_{config["å®¡åˆ¤ç¨‹åº"]}_{config["æ³•é™¢å±‚çº§"]}_{config["åˆ¤å†³ç»“æœ"]}_{config["åˆ¤å†³å¼€å§‹æ—¶é—´"]}_{config["åˆ¤å†³ç»“æŸæ—¶é—´"]}'
        create_dir(self.__base_dir)
        self.logger = logger

    async def get_url(
        self, target_page_queue: asyncio.Queue, session: aiohttp.ClientSession
    ) -> list:
        while not target_page_queue.empty():
            try:
                page = await target_page_queue.get()
                keyword_ = requests.utils.quote(self.keyword)
                url = f"{self.base_url}/search/judgement/advanced?showResults=true&keyword={keyword_}&causeId=&caseNo=&litigationType={self.litigation_type}&docType={self.doc_type}&litigant=&plaintiff=&defendant=&thirdParty=&lawyerId=&lawFirmId=&legals=&courtId=&judgeId=&clerk=&judgeDateYear=&judgeDateBegin={self.judge_date_begin}&judgeDateEnd={self.judge_date_end}&zone={self.zone}&procedureType={self.procedure_type}&lawId=&lawSearch=&courtLevel={self.court_level}&judgeResult={self.judge_result}&wordCount=&page={page}"
                self.logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬{page}é¡µ: {url}")
                async with session.get(url, headers=self.headers) as response:
                    response.raise_for_status()
                    text = await response.text()
                    self.logger.info(f"====== çˆ¬å–æˆåŠŸ, çŠ¶æ€ç : {response.status} ======")
            except Exception as e:
                self.logger.error(f"çˆ¬å–å¤±è´¥: {url}\n"
                      f"é”™è¯¯ä¿¡æ¯: {e}")
                await target_page_queue.put(page)
                raise e
            # æŠŠkeywordä¸­æ–‡è½¬æ¢æˆurlç¼–ç 
            # 1. åˆ›å»ºæ­£åˆ™è¡¨è¾¾å¼å¯¹è±¡
            pattern = re.compile(r"/judgement/[0-9a-z]*\?keyword=" + keyword_)
            # 2. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹è±¡åŒ¹é…text
            link_result = pattern.findall(text)
            link_result = [self.base_url + link for link in link_result]
            if link_result:
                self.links[page] = link_result

            await asyncio.sleep(0.15)

    async def crawl_links(self) -> bool:
        # é”™è¯¯å¤„ç†
        if self.start_page > self.end_page:
            self.logger.error(f"èµ·å§‹é¡µ{self.start_page}å¤§äºç»“æŸé¡µ{self.end_page}!!!")
            return False
        links = {}
        file_name = self.__base_dir + "/links.json"
        str_pages = list(
            str(i) for i in range(self.start_page, self.end_page + 1)
        )  # ["1", "2", "3", "4", "5", ...]
        target_pages = str_pages.copy()  # ["1", "2", "3", "4", "5", ...]
        # æƒ…å†µ: é“¾æ¥å·²ç»çˆ¬å–è¿‡äº†
        if os.path.exists(file_name):
            self.logger.info(f"======{file_name}å·²ç»å­˜åœ¨ï¼Œæ­£åœ¨æŸ¥çœ‹======")
            links = json.load(open(file_name, "r", encoding="utf-8"))  # è¯»å–åœ¨æœ¬åœ°çš„links
            self.links = links
            # ä»self.linksä¸­åˆ é™¤ä¸åœ¨target_pagesä¸­çš„page
            pages = list(self.links.keys())
            for page in pages:
                if page not in target_pages:
                    self.links.pop(page)
            for page in str_pages:
                if page in pages:
                    target_pages.remove(page)
        if len(target_pages) == 0:
            self.logger.info(f"======æ‰€æœ‰é“¾æ¥å·²ç»çˆ¬å–è¿‡äº†======")
            return True

        target_page_queue = asyncio.Queue()
        for page in target_pages:
            await target_page_queue.put(page)

        self.logger.info(f"======å¼€å§‹çˆ¬å–é“¾æ¥======")

        tasks = []
        temp_concurrent = self.concurrent
        while (not target_page_queue.empty()) and (temp_concurrent > 0):
            try:
                for i in range(temp_concurrent):
                    tasks.append(self.get_url(target_page_queue, self.session))
                await asyncio.gather(*tasks)
            # [Errno 54] Connection reset by peer
            except Exception as e:
                self.logger.error(e)
                self.logger.info(f"å¹¶å‘æ•°é‡è¶…è¿‡æœ€å¤§å€¼å¤ªå¤š, å‡å°‘å¹¶å‘æ•°é‡[{temp_concurrent} -> {temp_concurrent//2}]")
                temp_concurrent = temp_concurrent // 2

            asyncio.sleep(0.1)

        self.logger.info(f"======é“¾æ¥çˆ¬å–å®Œç»“æŸ=====")

        if self.links is None or len(self.links.keys()) == 0:
            self.logger.error(f"æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„é“¾æ¥ï¼Œè¯·æ›´æ”¹æœç´¢æ¡ä»¶ï¼")
            return False
        else:
            with open(file_name, "w", encoding="utf-8") as f:
                json.dump(self.links, f, ensure_ascii=False)
            self.logger.info(f"======{file_name}ä¿å­˜å®Œæˆ======")
            return True

    def filter_text(self, text) -> str:
        if text is None or len(text) == 0:
            return ""
        else:
            text = text[0]
            text = text.strip()
            # å»é™¤textä¸­çš„æ‰€æœ‰<a><\a>çš„å†…å®¹
            text = re.sub(r"<a.*?>|<\/a>", "", text)
            # å»é™¤textä¸­çš„æ‰€æœ‰<em><\em>çš„å†…å®¹
            text = re.sub(r"<em.*?>|<\/em>", "", text)
            # å»é™¤textä¸­çš„æ‰€æœ‰<span><\span>çš„å†…å®¹
            text = re.sub(r"<span.*?>|<\/span>", "", text)
            # å»é™¤textä¸­çš„æ‰€æœ‰<p><\p>çš„å†…å®¹
            text = re.sub(r"<p.*?>|<\/p>", "", text)
            # å»é™¤ &nbsp; &lt; &gt; &amp;
            text = re.sub(r"&lt;", "<", text)
            text = re.sub(r"&gt;", ">", text)
            text = re.sub(r"&amp;", "&", text)
            text = re.sub(r"&nbsp;", "", text)
            return text

    async def add_content(
        self, url_queue: asyncio.Queue, session: aiohttp.ClientSession
    ) -> None:
        while not url_queue.empty():
            try:
                content = {}
                url = await url_queue.get()
                self.logger.info(f"\n- æ­£åœ¨çˆ¬å–: {url}")
                async with session.get(url, headers=self.headers) as response:
                    text = await response.text()
            except Exception as e:
                await url_queue.put(url)
                raise e
            if text is None or len(text) == 0:
                self.logger.error(f"çˆ¬å–å¤±è´¥: {url}")
                continue
            soup = BeautifulSoup(text, "html.parser")
            content["é“¾æ¥"] = url
            try:
                content["æ ‡é¢˜"] = soup.find("h2", class_="entry-title").text.strip()  # æ ‡é¢˜
            except Exception as e:
                content["æ ‡é¢˜"] = ""
                print(f"æ ‡é¢˜çˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["æ—¥æœŸ"] = (
                    soup.find("li", class_="ht-kb-em-date")
                    .text.strip()
                    .split("æ—¥æœŸï¼š")[-1]
                )  # æ—¥æœŸ
            except Exception as e:
                content["æ—¥æœŸ"] = ""
                # print(f"æ—¥æœŸçˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["æ³•é™¢"] = (
                    soup.find("li", class_="ht-kb-em-author")
                    .text.strip()
                    .split("æ³•é™¢ï¼š")[-1]
                )  # æ³•é™¢
            except Exception as e:
                content["æ³•é™¢"] = ""
                # print(f"æ³•é™¢çˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["æ¡ˆå·"] = (
                    soup.find("li", class_="ht-kb-em-category")
                    .text.strip()
                    .split("æ¡ˆå·ï¼š")[-1]
                )  # æ¡ˆå·
            except Exception as e:
                content["æ¡ˆå·"] = ""
                # print(f"æ¡ˆå·çˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                litigants = soup.find("div", id="Litigants").wrap
                content["å½“äº‹äºº"] = re.findall(r"<p>(.*?)</p>", str(litigants))  # å½“äº‹äºº
            except Exception as e:
                content["å½“äº‹äºº"] = ""
                # print(f"å½“äº‹äººçˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["åº­å®¡ç¨‹åºè¯´æ˜"] = soup.find(
                    "div", id="Explain"
                ).text.strip()  # åº­å®¡ç¨‹åºè¯´æ˜
            except Exception as e:
                content["åº­å®¡ç¨‹åºè¯´æ˜"] = ""
                # print(f"åº­å®¡ç¨‹åºè¯´æ˜çˆ¬å–å¤±è´¥: {url}")
                # print(e)

            try:
                content["åº­å®¡è¿‡ç¨‹"] = soup.find("div", id="Procedure").text.strip()  # åº­å®¡è¿‡ç¨‹
            except Exception as e:
                content["åº­å®¡è¿‡ç¨‹"] = ""
                # print(f"åº­å®¡è¿‡ç¨‹çˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["æŸ¥æ˜äº‹å®"] = soup.find("div", id="Facts").text.strip()  # æŸ¥æ˜äº‹å®
            except Exception as e:  # æŸ¥æ˜äº‹å®
                content["æŸ¥æ˜äº‹å®"] = ""
                # print(f"æŸ¥æ˜äº‹å®çˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["æ³•é™¢æ„è§"] = soup.find("div", id="Opinion").text.strip()  # æ³•é™¢æ„è§
            except Exception as e:
                content["æ³•é™¢æ„è§"] = ""
                # print(f"æ³•é™¢æ„è§çˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["åˆ¤å†³ç»“æœ"] = soup.find("div", id="Verdict").text.strip()  # åˆ¤å†³ç»“æœ
            except Exception as e:
                content["åˆ¤å†³ç»“æœ"] = ""
                # print(f"åˆ¤å†³ç»“æœçˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["åº­åå‘ŠçŸ¥"] = soup.find("div", id="Inform").text.strip()  # åº­åå‘ŠçŸ¥
            except Exception as e:  # åº­åå‘ŠçŸ¥
                content["åº­åå‘ŠçŸ¥"] = ""
                # print(f"åº­åå‘ŠçŸ¥çˆ¬å–å¤±è´¥: {url}")
                # print(e)
            try:
                content["ç»“å°¾"] = soup.find("div", id="Ending").text.strip()  # ç»“å°¾
            except Exception as e:
                content["ç»“å°¾"] = ""
                # print(f"ç»“å°¾çˆ¬å–å¤±è´¥: {url}")
                # print(e)

            try:
                info = (
                    soup.find(
                        "section", class_="widget HT_KB_Authors_Widget clearfix"
                    ).text.strip()
                    + "\n"
                )  # æ¡ˆä»¶ä¿¡æ¯
            except Exception as e:
                info = ""

            if info:
                try:
                    content["ç±»å‹"] = (
                        re.findall(r"ç±»å‹ï¼š(.*)\n", info)[0]
                        if self.doc_type == ""
                        else self.doc_type
                    )
                except Exception as e:
                    content["ç±»å‹"] = ""
                try:
                    content["ç¨‹åº"] = (
                        re.findall(r"ç¨‹åºï¼š(.*)\n", info)[0]
                        if self.procedure_type == ""
                        else self.procedure_type
                    )
                except Exception as e:
                    content["ç¨‹åº"] = ""
                try:
                    content["åˆ¤å†³ç»“æœ"] = re.findall(r"åˆ¤å†³ç»“æœï¼š(.*)\n", info)[0]
                except Exception as e:
                    content["åˆ¤å†³ç»“æœ"] = ""
                try:
                    content["æ¶‰è¯‰æœºå…³ç±»å‹"] = re.findall(r"æ¶‰è¯‰æœºå…³ç±»å‹ï¼š(.*)\n", info)[0]
                except Exception as e:
                    content["æ¶‰è¯‰æœºå…³ç±»å‹"] = ""
                try:
                    content["æ¡ˆç”±"] = re.findall(r"æ¡ˆç”±ï¼š(.*)\n", info)[0]
                except Exception as e:
                    content["æ¡ˆç”±"] = ""
            else:
                content["ç±»å‹"] = ""
                content["ç¨‹åº"] = ""
                content["åˆ¤å†³ç»“æœ"] = ""
                content["æ¶‰è¯‰æœºå…³ç±»å‹"] = ""
                content["æ¡ˆç”±"] = ""
            try:
                content["æ ‡ç­¾"] = (
                    re.findall('<a href="[^"]+" rel="tag">(.*?)</a>', text)
                    if re.findall('<a href="[^"]+" rel="tag">(.*?)</a>', text)
                    else ""
                )
            except Exception as e:
                content["tags"] = ""
                # print(f"æ ‡ç­¾çˆ¬å–å¤±è´¥: {url}")
                # print(e)

            self.contents.append(content)

    async def crawl_contents(self):
        contents = []
        urls = []  # å·²ç»çˆ¬å–è¿‡çš„é“¾æ¥
        file_name = self.__base_dir + "/contents.json"
        target_urls = []
        for page, links in self.links.items():
            if len(target_urls) >= self.num:
                break
            target_urls.extend(links)

        self.logger.info(f"======å…±æœ‰{len(target_urls)}æ¡é“¾æ¥éœ€è¦çˆ¬å–======")

        
        # æ‰“å¼€æ–‡ä»¶æ£€æŸ¥æ˜¯å¦å·²ç»çˆ¬å–è¿‡
        if os.path.exists(file_name):
            contents = json.load(open(file_name, "r", encoding="utf-8"))
            self.contents = contents
            # å·²ç»çˆ¬å–è¿‡çš„é“¾æ¥
            urls = [content["é“¾æ¥"] for content in self.contents]
            # å»é™¤å·²çˆ¬å–è¿‡çš„å†…å®¹äº† é“¾æ¥ä¸å±äºself.linksçš„å†…å®¹
            for row in self.contents:
                if row["é“¾æ¥"] not in target_urls:
                    self.contents.remove(row)
            # å»é™¤å·²çˆ¬å–è¿‡çš„é“¾æ¥
            for url in urls:
                if url in target_urls:
                    target_urls.remove(url)

        # åˆ›å»ºé˜Ÿåˆ—
        url_queue = asyncio.Queue()
        for url in target_urls:
            if url not in urls:
                await url_queue.put(url)

        self.logger.info(f"======å¼€å§‹åˆ›å»ºçˆ¬å–ä»»åŠ¡======")
        temp_concurrent = self.concurrent
        tasks = []
        while (not url_queue.empty()) and (temp_concurrent > 0):
            try:
                for i in range(temp_concurrent):
                    tasks.append(self.add_content(url_queue, self.session))
                await asyncio.gather(*tasks)
            except Exception as e:
                self.logger.error(e)
                self.logger.info(f"å¹¶å‘æ•°é‡è¶…è¿‡æœ€å¤§å€¼å¤ªå¤š, å‡å°‘å¹¶å‘æ•°é‡[{temp_concurrent} -> {temp_concurrent//2}]")
                temp_concurrent = temp_concurrent // 2
            asyncio.sleep(0.1)

        self.logger.info(f"======å†…å®¹çˆ¬å–å®Œæˆ======")
        # ä¿å­˜
        with open(file_name, "w", encoding="utf-8") as f:
            json.dump(self.contents, f, ensure_ascii=False)
        self.logger.info(f"======{file_name}ä¿å­˜å®Œæˆ======")

    def save_to_excel(self):
        df = pd.DataFrame(self.contents)
        # ä¿å­˜åˆ°excel
        file_name = self.__base_dir + "/contents.xlsx"

        df.to_excel(file_name, index=False)
        self.logger.info(f"======{file_name}ä¿å­˜å®Œæˆ======")

    def basic_analysis(self):
        """
        åˆ†æï¼š
        ä¸€å®¡æ•°é‡ã€äºŒå®¡æ•°é‡ã€å†å®¡æ•°é‡ã€å¤æ ¸æ•°é‡ã€åˆ‘ç½šå˜æ›´æ•°é‡ã€å…¶ä»–æ•°é‡
        è£å®šä¹¦æ•°é‡ã€åˆ¤å†³ä¹¦æ•°é‡ã€è°ƒè§£ä¹¦æ•°é‡ã€å†³å®šä¹¦æ•°é‡ã€ä»¤æ•°é‡ã€é€šçŸ¥ä¹¦æ•°é‡ã€å…¶ä»–æ•°é‡
        æ¯ä¸ªæ¡ˆç”±çš„æ•°é‡
        æ¯ä¸ªæ ‡ç­¾çš„æ•°é‡
        """
        if not self.contents:
            return {}
        analysis = {}
        # ä¸€å®¡æ•°é‡ã€äºŒå®¡æ•°é‡ã€å†å®¡æ•°é‡ã€å¤æ ¸æ•°é‡ã€åˆ‘ç½šå˜æ›´æ•°é‡ã€å…¶ä»–æ•°é‡
        analysis["å®¡åˆ¤ç¨‹åº"] = {}

        for procedure_type in PROCEDURE_TYPE_MAP.keys():
            if procedure_type:
                analysis["å®¡åˆ¤ç¨‹åº"][procedure_type] = 0
        for content in self.contents:
            procedure_type = content["ç¨‹åº"]
            if procedure_type:
                if procedure_type in PROCEDURE_TYPE_MAP.keys():
                    analysis["å®¡åˆ¤ç¨‹åº"][procedure_type] += 1
                elif procedure_type in PROCEDURE_TYPE_MAP.values():
                    idx = list(PROCEDURE_TYPE_MAP.values()).index(procedure_type)
                    analysis["å®¡åˆ¤ç¨‹åº"][list(PROCEDURE_TYPE_MAP.keys())[idx]] += 1
                else:
                    continue
            else:
                continue
        # è£å®šä¹¦æ•°é‡ã€åˆ¤å†³ä¹¦æ•°é‡ã€è°ƒè§£ä¹¦æ•°é‡ã€å†³å®šä¹¦æ•°é‡ã€ä»¤æ•°é‡ã€é€šçŸ¥ä¹¦æ•°é‡ã€å…¶ä»–æ•°é‡
        analysis["æ–‡ä¹¦ç±»å‹"] = {}
        for doc_type in DOC_TYPE_MAP.keys():
            if doc_type:
                analysis["æ–‡ä¹¦ç±»å‹"][doc_type] = 0
        for content in self.contents:
            doc_type = content["ç±»å‹"]
            if doc_type:
                if doc_type in DOC_TYPE_MAP.keys():
                    analysis["æ–‡ä¹¦ç±»å‹"][doc_type] += 1
                elif doc_type in DOC_TYPE_MAP.values():
                    idx = list(DOC_TYPE_MAP.values()).index(doc_type)
                    analysis["æ–‡ä¹¦ç±»å‹"][list(DOC_TYPE_MAP.keys())[idx]] += 1
                else:
                    continue
            else:
                continue
        # æ¡ˆç”±
        analysis["æ¡ˆç”±"] = {}
        for content in self.contents:
            case = content["æ¡ˆç”±"]
            if case:
                if case not in analysis["æ¡ˆç”±"]:
                    analysis["æ¡ˆç”±"][case] = 0
                analysis["æ¡ˆç”±"][case] += 1
        # æ ‡ç­¾
        analysis["æ ‡ç­¾"] = {}
        for content in self.contents:
            tags = content["æ ‡ç­¾"]
            for tag in tags:
                if tag:
                    if tag not in analysis["æ ‡ç­¾"]:
                        analysis["æ ‡ç­¾"][tag] = 0
                    analysis["æ ‡ç­¾"][tag] += 1

        # æ’åº
        analysis["å®¡åˆ¤ç¨‹åº"] = dict(
            sorted(analysis["å®¡åˆ¤ç¨‹åº"].items(), key=lambda item: item[1], reverse=True)
        )
        analysis["æ–‡ä¹¦ç±»å‹"] = dict(
            sorted(analysis["æ–‡ä¹¦ç±»å‹"].items(), key=lambda item: item[1], reverse=True)
        )
        analysis["æ¡ˆç”±"] = dict(
            sorted(analysis["æ¡ˆç”±"].items(), key=lambda item: item[1], reverse=True)
        )
        analysis["æ ‡ç­¾"] = dict(
            sorted(analysis["æ ‡ç­¾"].items(), key=lambda item: item[1], reverse=True)
        )

        return analysis

    async def __ask_gpt(self, queue: asyncio.Queue):
        default_extraxtion_json = {
            "åŸå‘Šèµ·è¯‰çš„äº‹å®ä¸ç†ç”±": "",
            "åŸå‘Šèµ·è¯‰çš„æ³•å¾‹ä¾æ®": [],
            "åŸå‘Šèµ·è¯‰çš„è¯‰è®¼è¯·æ±‚": "",
            "è¢«å‘Šè¾©ç§°çš„äº‹å®ä¸ç†ç”±": "",
            "è¢«å‘Šè¾©ç§°çš„æ³•å¾‹ä¾æ®": [],
            "æ³•é™¢è®¤å®šå’ŒæŸ¥æ˜çš„äº‹å®": "",
            "æ³•é™¢çš„åˆ¤å†³çš„æ³•å¾‹ä¾æ®": [],
            "æ³•é™¢çš„åˆ¤å†³ç»“æœ": "",
        }
        while not queue.empty():
            content = await queue.get()
            self.logger.info(f"======ç›®å‰æ­£åœ¨å¤„ç† [{content['æ ‡é¢˜']}]======")
            if not content["åº­å®¡è¿‡ç¨‹"]:
                content.update(default_extraxtion_json)
                self.logger.info(f"======âœ…{content['æ ‡é¢˜']} æ²¡æœ‰åº­å®¡è¿‡ç¨‹======")
                continue
            chain = get_conversation_chain(
                model_name="gpt-3.5-turbo-16k-0613", prompt=LAW_RESULT_TEMPLATE
            )
            try:
                extraxtion = chain.predict(
                    explain=content["åº­å®¡ç¨‹åºè¯´æ˜"],
                    procedure=content["åº­å®¡è¿‡ç¨‹"],
                    facts=content["æŸ¥æ˜äº‹å®"],
                    opinion=content["æ³•é™¢æ„è§"],
                )
                extraxtion_json = json.loads(extraxtion)
                content.update(extraxtion_json)
            except Exception as e:
                extraxtion_json = default_extraxtion_json
                self.logger.error(f"======âŒ{content['æ ‡é¢˜']} AIå¤„ç†å¤±è´¥, é”™è¯¯ä¿¡æ¯: {e}======"
                                  "é»˜è®¤ä½¿ç”¨ç©ºçš„ç»“æœ")
            content.update(extraxtion_json)
            self.logger.info(f"======âœ…{content['æ ‡é¢˜']} AIå¤„ç†å®Œæˆ======")

    async def ai_process(self):
        if not self.contents:
            return
        self.logger.info("======å¼€å§‹AIå¤„ç†====")
        queue = asyncio.Queue()
        for content in self.contents:
            await queue.put(content)
        tasks = []
        temp_concurrent = self.concurrent // 2
        for i in range(temp_concurrent):
            tasks.append(self.__ask_gpt(queue))
        await asyncio.gather(*tasks)
        self.logger.info("======ğŸ¤–âœ…AIå¤„ç†å®Œæˆ======")

    @property
    def df(self):
        return pd.DataFrame(self.contents)

    @property
    def base_dir(self):
        return self.__base_dir

    @property
    def analysis(self):
        return self.basic_analysis()
