import streamlit as st
import pandas as pd
from io import BytesIO
from pyxlsb import open_workbook as open_xlsb
import xlsxwriter


from src.utils import set_env
set_env()

from datetime import date, datetime
from src.spider.openlaw_spider import (
    DOC_TYPE_MAP,
    PROCEDURE_TYPE_MAP,
    COURT_LEVEL_MAP,
    JUDGE_RESULT_MAP,
    LITIGATION_TYPE_MAP,
    ZONE_MAP,
)

import asyncio
import aiohttp
from codetiming import Timer
from src.spider import OpenLawSpider
from src.analysis import Analyzer
from src.utils import read_config

def to_excel(df):
    output = BytesIO()
    writer = pd.ExcelWriter(output, engine='xlsxwriter')
    df.to_excel(writer, index=False, sheet_name='Sheet1')
    workbook = writer.book
    worksheet = writer.sheets['Sheet1']
    format1 = workbook.add_format({'num_format': '0.00'}) 
    worksheet.set_column('A:A', None, format1)  
    writer.save()
    processed_data = output.getvalue()
    return processed_data


def set_session_state_step(i):
    st.session_state.step = i


async def main():
    if "step" not in st.session_state:
        st.session_state.step = 0

    config = {}
    if st.session_state.step >= 0:
        st.title("ğŸ“–OpenLawçˆ¬å–åŠ©æ‰‹")
        config["å…³é”®è¯"] = st.text_input("å…³é”®è¯", placeholder="è¯·è¾“å…¥å…³é”®è¯")
        config["æ¡ˆä»¶ç±»å‹"] = st.selectbox("æ¡ˆä»¶ç±»å‹", list(LITIGATION_TYPE_MAP.keys()))
        config["æ³•é™¢ï¼ˆåœ°åŒºï¼‰"] = st.selectbox("æ³•é™¢ï¼ˆåœ°åŒºï¼‰", list(ZONE_MAP.keys()))
        config["æ³•é™¢å±‚çº§"] = st.selectbox("æ³•é™¢å±‚çº§", list(COURT_LEVEL_MAP.keys()))
        config["å®¡åˆ¤ç¨‹åº"] = st.selectbox("å®¡åˆ¤ç¨‹åº", list(PROCEDURE_TYPE_MAP.keys()))
        config["æ–‡ä¹¦ç±»å‹"] = st.selectbox("æ–‡ä¹¦ç±»å‹", list(DOC_TYPE_MAP.keys()))
        config["åˆ¤å†³ç»“æœ"] = st.selectbox("åˆ¤å†³ç»“æœ", list(JUDGE_RESULT_MAP.keys()))
        config["cookie"] = st.text_input(
            "cookie", "SESSION=MzQxMTMyYjEtMjBkZC00NzQ0LWI0N2EtNzUyYjQzZDA4NDA2"
        )
        config[
            "user_agent"
        ] = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        start, end = st.columns(2)
        with start:
            config["åˆ¤å†³å¼€å§‹æ—¶é—´"] = st.date_input("åˆ¤å†³å¼€å§‹æ—¶é—´", date(2011, 1, 1))
        with end:
            config["åˆ¤å†³ç»“æŸæ—¶é—´"] = st.date_input("åˆ¤å†³ç»“æŸæ—¶é—´", datetime.now())
        page_num = st.number_input("æ‚¨å¸Œæœ›è‡³å°‘æœ‰å¤šå°‘è¿”å›ç»“æœï¼ˆä¸€é¡µ20æ¡ç»“æœï¼‰", 1, None, 100, 20)
        if page_num:
            config["start_page"] = 1
            config["end_page"] = (page_num-1) // 20 + 1

        st.button(
            "ğŸ˜€å¼€å§‹åˆ†æ",
            on_click=set_session_state_step,
            args=(1,),
            use_container_width=True,
        )
    if st.session_state.step >= 1:
        timer = Timer("timer", logger=None)
        timer.start()
        async with aiohttp.ClientSession() as session:
            # æ•°æ®çˆ¬å–
            spider = OpenLawSpider(
                config,
                session=session,
                concurrent=50,
            )
            with st.spinner("æ­£åœ¨çˆ¬å–é“¾æ¥ğŸ”—..."):
                # 1. çˆ¬å–é“¾æ¥,ä¿å­˜ä¸ºcsvæ–‡ä»¶
                have_links = await spider.crawl_links()

            if have_links:  # å¦‚æœçˆ¬å–æˆåŠŸ
                st.header("çˆ¬å–é“¾æ¥æˆåŠŸ")
                st.json(spider.links, expanded=False)

                with st.spinner("æ­£åœ¨çˆ¬å–å†…å®¹..."):
                    await spider.crawl_contents()
                timer.stop()
                st.success(f"ğŸ˜€çˆ¬å–å®Œæˆï¼Œè€—æ—¶{timer.last:.2f}ç§’")
                st.subheader("åŸºç¡€åˆ†æç»“æœ")
                for key, value in spider.analysis.items():
                    st.markdown(f"**{key}**")
                    if key == "æ ‡ç­¾":
                        st.json(value, expanded=False)
                    else:
                        st.json(value, expanded=True)

                # aiæå–
                with st.spinner("æ­£åœ¨AIæå–ä¿¡æ¯ï¼Œè¯·è€å¿ƒç­‰å¾…..."):
                    await spider.ai_process()
                st.subheader("AIæå–ä¿¡æ¯æˆåŠŸ")
                
                df_xlsx = to_excel(spider.df)
                file_name = spider.base_dir + ".xlsx"
                st.download_button(label='ğŸ“¥ ä¸‹è½½ç»“æœ',
                                                data=df_xlsx ,
                                                file_name= file_name)
                st.header(f"çˆ¬å–å†…å®¹æˆåŠŸ[å…±{len(spider.contents)}æ¡]")
                for content in spider.contents:
                    st.json(content, expanded=True)


if __name__ == "__main__":
    asyncio.run(main())
